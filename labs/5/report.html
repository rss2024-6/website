<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, user-scalable=yes"
    />
    <title>report</title>
    <style>
      html {
        color: #1a1a1a;
        background-color: #fdfdfd;
      }
      body {
        margin: 0 auto;
        /* max-width: 36em; */
        padding-left: 50px;
        padding-right: 50px;
        padding-top: 50px;
        padding-bottom: 50px;
        hyphens: auto;
        overflow-wrap: break-word;
        text-rendering: optimizeLegibility;
        font-kerning: normal;
      }
      @media (max-width: 600px) {
        body {
          font-size: 0.9em;
          padding: 12px;
        }
        h1 {
          font-size: 1.8em;
        }
      }
      @media print {
        html {
          background-color: white;
        }
        body {
          background-color: transparent;
          color: black;
          font-size: 12pt;
        }
        p,
        h2,
        h3 {
          orphans: 3;
          widows: 3;
        }
        h2,
        h3,
        h4 {
          page-break-after: avoid;
        }
      }
      p {
        margin: 1em 0;
      }
      a {
        color: #1a1a1a;
      }
      a:visited {
        color: #1a1a1a;
      }
      img {
        max-width: 100%;
      }
      svg {
        height: auto;
        max-width: 100%;
      }
      h1,
      h2,
      h3,
      h4,
      h5,
      h6 {
        margin-top: 1.4em;
      }
      h5,
      h6 {
        font-size: 1em;
        font-style: italic;
      }
      h6 {
        font-weight: normal;
      }
      ol,
      ul {
        padding-left: 1.7em;
        margin-top: 1em;
      }
      li > ol,
      li > ul {
        margin-top: 0;
      }
      blockquote {
        margin: 1em 0 1em 1.7em;
        padding-left: 1em;
        border-left: 2px solid #e6e6e6;
        color: #606060;
      }
      code {
        font-family: Menlo, Monaco, Consolas, "Lucida Console", monospace;
        font-size: 85%;
        margin: 0;
        hyphens: manual;
      }
      pre {
        margin: 1em 0;
        overflow: auto;
      }
      pre code {
        padding: 0;
        overflow: visible;
        overflow-wrap: normal;
      }
      .sourceCode {
        background-color: transparent;
        overflow: visible;
      }
      hr {
        background-color: #1a1a1a;
        border: none;
        height: 1px;
        margin: 1em 0;
      }
      table {
        margin: 1em 0;
        border-collapse: collapse;
        width: 100%;
        overflow-x: auto;
        display: block;
        font-variant-numeric: lining-nums tabular-nums;
      }
      table caption {
        margin-bottom: 0.75em;
      }
      tbody {
        margin-top: 0.5em;
        border-top: 1px solid #1a1a1a;
        border-bottom: 1px solid #1a1a1a;
      }
      th {
        border-top: 1px solid #1a1a1a;
        padding: 0.25em 0.5em 0.25em 0.5em;
      }
      td {
        padding: 0.125em 0.5em 0.25em 0.5em;
      }
      header {
        margin-bottom: 4em;
        text-align: center;
      }
      #TOC li {
        list-style: none;
      }
      #TOC ul {
        padding-left: 1.3em;
      }
      #TOC > ul {
        padding-left: 0;
      }
      #TOC a:not(:hover) {
        text-decoration: none;
      }
      code {
        white-space: pre-wrap;
      }
      span.smallcaps {
        font-variant: small-caps;
      }
      div.columns {
        display: flex;
        gap: min(4vw, 1.5em);
      }
      div.column {
        flex: auto;
        overflow-x: auto;
      }
      div.hanging-indent {
        margin-left: 1.5em;
        text-indent: -1.5em;
      }
      /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
      ul.task-list[class] {
        list-style: none;
      }
      ul.task-list li input[type="checkbox"] {
        font-size: inherit;
        width: 0.8em;
        margin: 0 0.8em 0.2em -1.6em;
        vertical-align: middle;
      }
    </style>
    <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
    <script
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
      type="text/javascript"
    ></script>
  </head>
  <body>
    <nav id="TOC" role="doc-toc">
      <ul>
        <li>
          <a
            href="#lab-5-report-monte-carlo-localization-mcl"
            id="toc-lab-5-report-monte-carlo-localization-mcl"
            >LAB 5 REPORT: Monte Carlo Localization (MCL)</a
          >
          <ul>
            <li>
              <a href="#i.-introduction" id="toc-i.-introduction"
                >I. Introduction</a
              >
            </li>
            <li>
              <a href="#ii.-technical-approach" id="toc-ii.-technical-approach"
                >II. Technical Approach</a
              >
              <ul>
                <li>
                  <a href="#a.-motion-model" id="toc-a.-motion-model"
                    >A. Motion Model</a
                  >
                </li>
                <li>
                  <a href="#b.-sensor-model" id="toc-b.-sensor-model"
                    >B. Sensor Model</a
                  >
                </li>
                <li>
                  <a href="#c.-particles-filter" id="toc-c.-particles-filter"
                    >C. Particles Filter</a
                  >
                </li>
                <li>
                  <a
                    href="#d.-robot-implementation"
                    id="toc-d.-robot-implementation"
                    >D. Robot Implementation</a
                  >
                </li>
              </ul>
            </li>
            <li>
              <a
                href="#iii.-experimental-evaluation"
                id="toc-iii.-experimental-evaluation"
                >III. Experimental Evaluation</a
              >
              <ul>
                <li>
                  <a
                    href="#a.-performance-in-simulation"
                    id="toc-a.-performance-in-simulation"
                    >A. Performance in Simulation</a
                  >
                </li>
                <li>
                  <a
                    href="#b.-performance-in-the-real-world"
                    id="toc-b.-performance-in-the-real-world"
                    >B. Performance in the Real World</a
                  >
                </li>
              </ul>
            </li>
            <li>
              <a href="#iv.-conclusion" id="toc-iv.-conclusion"
                >IV. Conclusion</a
              >
              <ul>
                <li>
                  <a href="#lessons-learned" id="toc-lessons-learned"
                    >Lessons Learned</a
                  >
                </li>
              </ul>
            </li>
            <li><a href="#references" id="toc-references">References</a></li>
          </ul>
        </li>
      </ul>
    </nav>
    <h1 id="lab-5-report-monte-carlo-localization-mcl">
      LAB 5 REPORT: Monte Carlo Localization (MCL)
    </h1>
    <h2 id="i.-introduction">I. Introduction</h2>
    <p><em>Author: Rulan Gu</em></p>
    <p>
      In autonomous vehicles, location estimation is a very useful feature when
      attempting to navigate through a mapped environment. Autonomous cars, for
      example, use a GPS system to know where they are in the real world, which
      enables them to plan routes to a location and anticipate upcoming
      obstacles and turns that cameras or sensors wouldn’t be able to detect.
      Thus, implementing a location estimation feature would greatly contribute
      to our overall goal of designing an autonomous car.
    </p>
    <p>
      So far, we have implemented some autonomous routines for our robot: it can
      park at a cone and follow walls and orange lines. However, it still can’t
      navigate through a known area (such as the State basement). In Lab 5, our
      goal was to implement an algorithm to estimate the robot’s current
      position on the map (in our case, a map of the State basement) in order to
      enable future features such as path planning.
    </p>
    <p>
      To accomplish our goal, we designed a Monte Carlo Localization (MCL)
      algorithm, which averages a probability distribution of particles that
      represent possible locations of the robot to find the estimated location.
      This algorithm is composed of three main parts:
    </p>
    <ul>
      <li>
        Motion model: Calculates the next set of particles (where the robot
        might be next) based on the robot’s odometry and previous set of
        particles.
      </li>
      <li>
        Sensor model: Uses laser scan data to calculate the probability of the
        robot being at each particle.
      </li>
      <li>
        Particle filter:
        <ul>
          <li>Updates particles when motion model is updated</li>
          <li>
            Resamples particles based on the probabilities calculated in sensor
            model
          </li>
          <li>
            Each time particles is updated by either model, publishes the new
            estimated robot pose, which is the average of all the particles.
          </li>
        </ul>
      </li>
    </ul>
    <p>
      In our implementation, we first initialize a particle cloud based around
      the robot’s initial position. Then, the particle filter runs repeatedly,
      starting with the initial particle cloud and updating estimated pose each
      time the motion or sensor models update.
    </p>
    <h2 id="ii.-technical-approach">II. Technical Approach</h2>
    <p><em>Author: Aimee Liu</em></p>
    <p>
      This section details the methods used to develop a working Monte Carlo
      Localization, or particle filter, using a motion model and sensor model to
      update the particle positions and determine the probability of each
      position. This particle filter is then implemented into our physical robot
      car in order to capture a real-time representation of the robot’s location
      on a map as it drives around the basement of the MIT Stata Center.
    </p>
    <p>
      Errors compound quickly when the robot loses track of its location —
      therefore, to maintain the robustness of the robot’s representation on the
      map, each component of the designed model must be optimized for accuracy
      and speed.
    </p>
    <h3 id="a.-motion-model">A. Motion Model</h3>
    <p><em>Author: Aimee Liu</em></p>
    <p>
      As the robot drives about in its environment, its detected location must
      change along with its movement. This process can be implemented with a
      motion model, which uses the sensed change in position, or odometry, of
      the robot to update its position on the map. In Monte Carlo Localization,
      the robot is represented as an array of particles representing potential
      positions, so the motion model must update the position of each particle
      with some defined noise to capture the true pose.
    </p>
    <p><strong>Updating Position with Transformation Matrices</strong></p>
    <p>
      One of the simplest ways to update the position of an object with its
      odometry is by using transformation matrices, which can be derived from
      the object’s pose vector, <span class="math inline">\(\bold{x}\)</span>,
      as represented by (1).
    </p>
    <p>
      <span class="math display">\[ \bold{x} = [x,y,\theta]^T \tag{1} \]</span>
    </p>
    <p>
      Here, <span class="math inline">\(x\)</span> and
      <span class="math inline">\(y\)</span> represent the object’s 2D
      coordinates and <span class="math inline">\(\theta\)</span> represents its
      orientation. This pose vector can be used to create a transformation
      matrix <span class="math inline">\(T\)</span>, through formula, (2).
    </p>
    $$ T =
    <span class="math display"
      >\[\begin{bmatrix} R &amp; p \\ 0 &amp; 1 \end{bmatrix}\]</span
    >
    =
    <span class="math display"
      >\[\begin{bmatrix} \cos(\theta) &amp; -\sin(\theta) &amp; x \\
      \sin(\theta) &amp; \cos(\theta) &amp; y \\ 0 &amp; 0 &amp; 1 \end{bmatrix}
      \tag{2}\]</span
    >
    <p>$$</p>
    <p>
      We can use this method to construct the transformation matrix of the
      object at timestep <span class="math inline">\(k-1\)</span>,
      <span class="math inline">\(T_{x_{k-1}}\)</span>, and the odometry,
      <span class="math inline">\(T_{\Delta x}\)</span>, in order to find the
      updated location of the object at timestep
      <span class="math inline">\(k\)</span>,
      <span class="math inline">\(T_{x_{k}}\)</span>, using formula (3).
    </p>
    <p>
      <span class="math display"
        >\[ T_{x_k} = T_{x_{k-1}}T_{\Delta x} \tag{3} \]</span
      >
    </p>
    <p>
      From, <span class="math inline">\(T_{x_{k}}\)</span>, we can extract
      entries <span class="math inline">\((1,3), (2, 3), (2, 1)\)</span>, and
      <span class="math inline">\((1, 1)\)</span>, to get
      <span class="math inline">\(x\)</span>,
      <span class="math inline">\(y\)</span>,
      <span class="math inline">\(sin(\theta)\)</span>, and
      <span class="math inline">\(cos(\theta)\)</span>, from which we can also
      get and
      <span class="math inline"
        >\(\theta = atan(\frac{sin(\theta)}{cos(\theta})\)</span
      >
      for the next pose.
    </p>
    <p><strong>Adding Noise to the Odometry</strong></p>
    <p>
      Due to various inaccuracies from sensors and the environment, our robot
      may never be able to identify its exact position and orientation on a map.
      Thus, it is instead represented by an array of particles which show its
      possible locations on the map. These particles are updated to move with
      odometry input, but the estimated odometry received by the robot’s sensors
      is not often precise. Therefore, noise drawn from a normal distribution
      centered around this received estimate must be added to spread the
      particles enough to capture the true odometry.
    </p>
    <p>
      This noise must be carefully tuned so that the particles are capable of
      converging to the true odometry while retaining as much information from
      the estimate as possible. As demonstrated by <em>Fig. 1</em>, a low spread
      would be too densely packed in one location to capture the true odometry
      if the sensor odometry is not accurate enough and a high spread would be
      too far apart to consistently find or get close to the true odometry. A
      good spread well encompasses the true odometry, with multiple particles
      within the vicinity that can get as close to the true odometry as
      possible.
    </p>
    <figure>
      <img
        src="imgs/Lab%205%20Report%20bf4998b43c0c4d6fa686559fbbb64070/fd385321-bcf3-451e-85cb-99c135a5a615.png"
        alt="Fig. 1. The optimal spread is between the low and high spread, where the particles are able to consistently encompass and get close to the true odometry with its variance. The robot is represented by the black box, the particles are represented by the red dots, and the true odometry is represented by the blue dot. In the depiction of a Low Spread, the particles are too densely packed in one location, and therefore too far from the true odometry to converge. In the depiction of a High Spread, the particles are too far apart to accurately find the true odometry and converge. The optimal spread would best be used to converge to the true odometry."
      />
      <figcaption aria-hidden="true">
        <strong><em>Fig. 1.</em></strong>
        <strong
          >The optimal spread is between the low and high spread, where the
          particles are able to consistently encompass and get close to the true
          odometry with its variance.</strong
        >
        The robot is represented by the black box, the particles are represented
        by the red dots, and the true odometry is represented by the blue dot.
        In the depiction of a Low Spread, the particles are too densely packed
        in one location, and therefore too far from the true odometry to
        converge. In the depiction of a High Spread, the particles are too far
        apart to accurately find the true odometry and converge. The optimal
        spread would best be used to converge to the true odometry.
      </figcaption>
    </figure>
    <p>
      <strong><em>Fig. 1.</em></strong>
      <strong
        >The optimal spread is between the low and high spread, where the
        particles are able to consistently encompass and get close to the true
        odometry with its variance.</strong
      >
      The robot is represented by the black box, the particles are represented
      by the red dots, and the true odometry is represented by the blue dot. In
      the depiction of a Low Spread, the particles are too densely packed in one
      location, and therefore too far from the true odometry to converge. In the
      depiction of a High Spread, the particles are too far apart to accurately
      find the true odometry and converge. The optimal spread would best be used
      to converge to the true odometry.
    </p>
    <p>
      The optimal spread we chose which met the criteria was
      <span class="math inline">\(\epsilon_x = 0.3\)</span>,
      <span class="math inline">\(\epsilon_y = 0.3\)</span>, and
      <span class="math inline">\(\epsilon_\theta = 0.005\)</span>.
    </p>
    <p>
      <strong>Synthesis: Updating Each Particle with Noisy Odometry</strong>
    </p>
    <p>
      For every timestep, each of the
      <span class="math inline">\(N\)</span> particles representing the robot’s
      position is updated to its next pose using an odometry with added noise
      using the transformation matrix method described earlier in this section.
      However, this may become computationally inefficient. To improve the speed
      of our model, all N noisy odometry vectors are created at once as a 3xN
      matrix and each computation is performed only once as an N-sized array of
      all the particles. This ensures that instead of each particle being
      updated one-by-one in series, the particles are all updated simultaneously
      in parallel, greatly reduceing the computation time of the model.
    </p>
    <p>
      This motion model constructs a spread of particle that consistently
      encompasses the true odometry to accurately predict the next position of
      the robot on the map, and is optimized to be quick and computationally
      efficient to keep up with the car.
    </p>
    <h3 id="b.-sensor-model">B. Sensor Model</h3>
    <p><em>Author: Sruthi Parthasarathi</em></p>
    <p>
      The motion model therefore gives us a space of possible locations for the
      car, but what we actually require is something more — while all of these
      locations are possible, they are not equally likely. Therefore, what we
      truly desire is a probability distribution across that space.
    </p>
    <hr />
    <p><strong>Particle Likelihoods</strong></p>
    <p>
      In order to do this, we start by asking ourselves the following: given a
      scan received in live time from the robot, how likely is the robot to have
      the pose of a given particle? Answering this allows us to determine an
      updated probability distribution for the true location of the robot from
      the current probability distribution, which is represented by the samples
      in the particle cloud.
    </p>
    <p>
      This probability is proportional to the conditional probability of
      observing the scan given the pose of the particle— since the particle
      probabilities are normalized in later steps, we compute this quantity
      instead for each of the particles in the cloud and absorb the constant of
      proportionality into the normalization constant, shown in (4).
    </p>
    <p>
      <span class="math display"
        >\[ P(p_k|z_{1,...,n}) \propto P(z_{1,...,n}|p_k) = \prod_{i=1}^n
        P(z_i|p_k) \tag{4} \]</span
      >
    </p>
    <p>
      Each individual scan data’s probability is given by the following formula,
      (5), with <span class="math inline">\(\alpha\)</span>’s chosen to add to
      1, and probability values as defined in (6), (7), (8), and (9).
    </p>
    <p>
      <span class="math display"
        >\[ P(z_i|p_k) = \alpha_{hit}p_{hit} + \alpha_{short}p_{short} +
        \alpha_{max}p_{max} + \alpha_{rand}p_{rand} \tag{5} \]</span
      >
    </p>
    <p>
      <span class="math display"
        >\[ p_{hit}(z_i|x_k, m) = \begin{cases}
        \eta\frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{(z_i - d)^2}{2\sigma^2})
        &amp; \text{if $0 \leq z_i \leq z_{max}$} \\ 0 &amp;
        \text{otherwise}\end{cases} \tag{6} \]</span
      >
    </p>
    <p>
      <span class="math display"
        >\[ p_{short}(z_i|x_k, m) = \frac{2}{d}\begin{cases} 1 - \frac{z_i}{d}
        &amp; \text{if $0 \leq z_i \leq d$ and $d \neq 0$} \\ 0 &amp;
        \text{otherwise}\end{cases} \tag{7} \]</span
      >
    </p>
    <p>
      <span class="math display"
        >\[ p_{max}(z_i|x_k, m) = \begin{cases} \frac{1}{\epsilon} &amp;
        \text{if $z_{max} - \epsilon \leq z_i \leq z_{max}$} \\ 0 &amp;
        \text{otherwise}\end{cases} \tag{8} \]</span
      >
    </p>
    <p>
      <span class="math display"
        >\[ p_{rand}(z_i|x_k, m) = \frac{2}{d}\begin{cases} 1 - \frac{z_i}{d}
        &amp; \text{if $0 \leq z_i \leq d$ and $d \neq 0$} \\ 0 &amp;
        \text{otherwise}\end{cases} \tag{9} \]</span
      >
    </p>
    <p>
      For a particle <span class="math inline">\(p_k\)</span> and scan range
      <span class="math inline">\(z_i\)</span>,
      <span class="math inline">\(x_i\)</span> is the particle’s position,
      <span class="math inline">\(m\)</span> is the map, and
      <span class="math inline">\(d\)</span> is the true distance for that scan
      data point.
    </p>
    <p><strong>Eliminating Repetitive Computations</strong></p>
    <p>
      Once we choose appropriate values for the alpha parameters, we can
      directly compute <span class="math inline">\(P(z_{1,...,k}|p)\)</span> for
      each particle. However, this process becomes computationally expensive
      very quickly when processing scans from the car in real time.
    </p>
    <figure>
      <img
        src="imgs/Lab%205%20Report%20bf4998b43c0c4d6fa686559fbbb64070/Precompute_Model.png"
        alt="Fig. 2. The precomputed sensor model for pre-chosen alpha values. Rather than compute probabilities for each particle in real-time as the sensor sends data, it is more computationally efficient to discretize and precompute the values beforehand and pull these values when needed."
      />
      <figcaption aria-hidden="true">
        <strong
          ><em>Fig. 2.</em> The precomputed sensor model for pre-chosen alpha
          values.</strong
        >
        Rather than compute probabilities for each particle in real-time as the
        sensor sends data, it is more computationally efficient to discretize
        and precompute the values beforehand and pull these values when needed.
      </figcaption>
    </figure>
    <p>
      <strong
        ><em>Fig. 2.</em> The precomputed sensor model for pre-chosen alpha
        values.</strong
      >
      Rather than compute probabilities for each particle in real-time as the
      sensor sends data, it is more computationally efficient to discretize and
      precompute the values beforehand and pull these values when needed.
    </p>
    <p>
      To combat this, we trade some accuracy for speed by storing a table of
      precomputed probabilities. Note that each probability only requires
      knowing the corresponding measured distance
      <span class="math inline">\(z\)</span> and the ground truth
      <span class="math inline">\(d\)</span>, and is unaffected by choice of
      units (changes in the distribution due to scaling are accounted for by the
      standard deviation). Therefore, our table stores probabilities for all
      pairs <span class="math inline">\((z, d)\)</span> where the two parameters
      are integers and range from <span class="math inline">\(0\)</span> pixels
      to <span class="math inline">\(200\)</span> pixels. As the scale from the
      real world to the pixels is fixed, the robot can simply use the scaling
      factor to convert its measurements to pixels, round to the nearest
      integer, and index into the table accordingly rather than repeatedly
      evaluating the functions above.
    </p>
    <p>
      And this is precisely what the robot does. By downsampling the laser scans
      to additionally reduce computational overhead, the sensor model calculates
      the likelihood values for each particle in the cloud, which the particle
      filter can then normalize to transform the space of possible positions
      into an actual probability distribution for the ground truth pose.
    </p>
    <p><em>Edited by Aimee Liu</em></p>
    <h3 id="c.-particles-filter">C. Particles Filter</h3>
    <p><em>Author: Tyrin Todd</em></p>
    <p>
      The particles filter integrates the sensor model and the motion model to
      get a predicted pose. On a high level, it follows a 4 step process to
      achieve this.
    </p>
    <ol type="1">
      <li>
        Get an initial distribution of particles based on ground ground truth
        data.
      </li>
      <li>Use laser scan and sensor model to update particle distribution</li>
      <li>Calculate new particles using motion model and predicted odometry</li>
      <li>Each time particles is updated, publish an average pose</li>
    </ol>
    <p><strong>Step 1: Initialize particles</strong></p>
    <p>
      Initializing the particles is fairly straightforward. The robot always has
      access to some ground truth pose, therefore we can use a normal
      distribution centered at the ground truth pose with a variance,
      <span class="math inline">\(\sigma\)</span>, for the initial particle
      values, as demonstrated by <em>Fig. 2</em>. We decided to use a variance
      of 0.1 because it generated a satisfactory and wide spread of particles
      around the robot.
    </p>
    <figure>
      <img
        src="imgs/standard_dev.png"
        alt="Fig. 3. Gaussian Distribution shows how different particle values (x,y,theta) are normally distributed by frequency."
      />
      <figcaption aria-hidden="true">
        <strong><em>Fig. 3.</em></strong>
        <strong>Gaussian Distribution</strong> shows how different particle
        values (x,y,theta) are normally distributed by frequency.
      </figcaption>
    </figure>
    <p>
      <strong><em>Fig. 3.</em></strong>
      <strong>Gaussian Distribution</strong> shows how different particle values
      (x,y,theta) are normally distributed by frequency.
    </p>
    <p>
      <strong>Step 2: Update particle distribution based on laser scan</strong>
    </p>
    <p>
      The next step involves adjusting the particle distribution based on
      incoming LiDAR data. Both the particles and the laser scan are processed
      through the sensor model, as described in <em>Section 2.B.</em>, which
      generates a list of probabilities for each particle’s accuracy. These
      probabilities are then used to resample particles, ensuring that particles
      with higher probabilities are represented more frequently than those with
      lower probabilities.
    </p>
    <p>
      <strong>Step 3: Update particle distribution based on odometry</strong>
    </p>
    <p>
      In step 3, the current odometry data is calculated. Then, the motion
      model, described in <em>Section 2.A.</em>, is applied to the current
      particle distribution and odometry, yielding a new particle distribution
      for the next timestep. The odometry calculation involves converting
      continuous-time velocity, <span class="math inline">\(v\)</span>,
      measurements into their displacement counterparts by multiplying them by
      <span class="math inline">\(dt\)</span>, the change in time to find the
      change in the <span class="math inline">\(x\)</span>,
      <span class="math inline">\(y\)</span>, and
      <span class="math inline">\(\theta\)</span> positions;
      <span class="math inline">\(dx\)</span>,
      <span class="math inline">\(dy\)</span>, and
      <span class="math inline">\(d\theta\)</span> respectively; as shown in
      (10).
    </p>
    <p>
      <span class="math display"
        >\[ dx = v_x*dt \tag{10}\\ dy = v_y*dt\\ d\theta = v_{\theta}*dt
        \]</span
      >
    </p>
    <p>
      There were multiple options to get velocity in this equation, such as
      using the current velocity or the previous. To take both velocities into
      account, we decided to use the average of the two velocity to calculate
      odometry for an updated formula, (11).
    </p>
    <p>
      <span class="math display"
        >\[ dx = (v_x-v_{x0})/2*dt\\ dy = (v_y-v_{y0})/2*dt\\ d\theta =
        (v_{\theta}-v_{\theta0})/2*dt \tag{11} \]</span
      >
    </p>
    <p><strong>Step 4: Update pose</strong></p>
    <p>
      Every time the particles are updated an average must be calculated.
      Unfortunately, this is not trivial as it seems. As described in the lab
      write up, “an average could pick a very unlikely pose between two modes”
      [1]. For example if the data were to look like <em>Fig. 4</em>, the
      algorithm inaccurately predicts our pose at the red triangle which has a
      low probability distribution and is therefore likely inaccurate. Our model
      is almost certain the robot is not there.
    </p>
    <figure>
      <img
        src="imgs/Lab%205%20Report%20bf4998b43c0c4d6fa686559fbbb64070/1e1c133f-0074-4564-9cb4-6037ec143853.png"
        alt="Fig. 4. Improved predicted position using mode clustering. The dots are now grouped into different colored clusters. The algorithm chooses the cluster with the most particles, in green, and uses the mean of cluster, resulting in an improved prediction of where the robot may be, as there is a high probability distribution at its location."
      />
      <figcaption aria-hidden="true">
        <strong
          ><em>Fig. 4.</em> Improved predicted position using mode
          clustering.</strong
        >
        The dots are now grouped into different colored clusters. The algorithm
        chooses the cluster with the most particles, in green, and uses the mean
        of cluster, resulting in an improved prediction of where the robot may
        be, as there is a high probability distribution at its location.
      </figcaption>
    </figure>
    <p>
      <strong
        ><em>Fig. 4.</em> Improved predicted position using mode
        clustering.</strong
      >
      The dots are now grouped into different colored clusters. The algorithm
      chooses the cluster with the most particles, in green, and uses the mean
      of cluster, resulting in an improved prediction of where the robot may be,
      as there is a high probability distribution at its location.
    </p>
    <p>
      Instead a better approach would be to split the data set into “clusters”.
      The biggest cluster will correspond to an area that our model is the most
      confident the robot is currently located in. Then we can take the average
      of this cluster to calculate the pose. As you can see in <em>Fig.5</em>,
      the predicted position is located in an area where the model has high
      confidence (The cluster with the most particles).
    </p>
    <p>
      <img
        src="imgs/Lab%205%20Report%20bf4998b43c0c4d6fa686559fbbb64070/2b0a02e5-6a64-4f08-a847-4e6e5838dc3e.png"
        alt="mode clustering"
      />
      <strong
        ><em>Fig. 5.</em> Improved predicted position using mode
        clustering.</strong
      >
      The dots are now grouped into different colored clusters. The algorithm
      chooses the cluster with the most particles, in green, and uses the mean
      of cluster, resulting in an improved prediction of where the robot may be,
      as there is a high probability distribution at its location.
    </p>
    <p>
      This procedure creates a robust particle filter that can accurately
      estimate the location of the robot on a map, being less susceptible to
      outliers and errors in the sensor data, and quickly update due to the
      reduced computation time of the motion and sensor models. The success of
      the particle filter in simulation is evaluated in <em>Section 3.A</em>.
    </p>
    <p><em>Edited by Aimee Liu</em></p>
    <h3 id="d.-robot-implementation">D. Robot Implementation</h3>
    <p><em>Author: Aimee Liu</em></p>
    <p>
      In order to implement the particle filter in the physical robot, we must
      change the parameter values used for the robot to instead use the physical
      robot parameters. The topics the robot reads for the particle filter is
      changed to read from the real robot’s LiDAR scan and odometry. In
      addition, we must also reverse the directions of the
      <span class="math inline">\(x\)</span>,
      <span class="math inline">\(y\)</span>, and
      <span class="math inline">\(\theta\)</span> axes since the robot views
      these values in the opposite direction. Performing these two changes
      prepares the particle filter to be used in the real world.
    </p>
    <p>
      However, before the translation from simulation to the real world may not
      always be very exact, as there is more noise and sensor errors that may
      arise on the physical robot that we typically don’t need to be concerned
      with on the simulation. Therefore, more fine tuning is necessary for the
      variances in the motion model or clustering in the particle filter.
      Despite this, our original values tuned to the simulation data still
      worked well on the physical robot and did not need as much adjustment, so
      our values remained relatively the same. The success of the particle
      filter in the robot is evaluated in <em>Section 3.B</em>.
    </p>
    <h2 id="iii.-experimental-evaluation">III. Experimental Evaluation</h2>
    <p><em>Author: Aimee Liu</em></p>
    <p>
      This section evaluates the robustness of our model in both simulation and
      reality. With the particle filter completed and implemented in the robot,
      it is important to verify the accuracy of our algorithm in tracking the
      robot’s position on a map. To do this, we tested the particle filter on
      both the simulation and physical car using a predetermined map of the MIT
      Stata basement. From this we can use different methods to evaluate the
      effectiveness of our algorithm both qualitatively and quantitatively.
    </p>
    <h3 id="a.-performance-in-simulation">A. Performance in Simulation</h3>
    <p><em>Author: Bradyn, Ty</em></p>
    <p>
      An advantage of operating in simulation is that the ground truth for the
      position of the robot is known. This allows for testing the accuracy of
      the localization algorithm. The distance from the localization algorithm’s
      expected position is compared to the true position of the robot to
      represent the error and plotted over time. We can see that the error is
      higher during times with higher velocity. Though the error went up during
      the middle of the movement, it is promising that the error goes back down
      to near zero once the robot comes to a stop, showing that the algorithm
      can converge back onto the correct solution, which is demonstrated in
      <em>Fig. 6</em>.
    </p>
    <p>
      <a
        href="https://drive.google.com/file/d/1qxt23u8WpZIM8nb53QbIi9XYlA9mo_cr/view?usp=drive_link"
        >https://drive.google.com/file/d/1qxt23u8WpZIM8nb53QbIi9XYlA9mo_cr/view?usp=drive_link</a
      >
    </p>
    <figure>
      <img
        src="imgs/accuracy.png"
        alt="Screenshot 2024-04-10 at 9.09.43 PM.png"
      />
      <figcaption aria-hidden="true">
        Screenshot 2024-04-10 at 9.09.43 PM.png
      </figcaption>
    </figure>
    <p>
      <strong
        ><em>Fig. 6.</em> Error between localization position estimate and
        ground truth.</strong
      >
      The x axis represents time with the y axis being the L2 norm of the (x, y)
      values for each.
    </p>
    <p>
      For our model to be robust the particles must remain well dispersed.
      Convergence of the particles leaves the model unable to cover enough area
      to be flexible to error. Similarly, divergence leaves the model too
      general and unable to focus on poses that matter. To track the how
      dispersed the particles were and how fast the model is converging we used
      the variance as shown in <em>Fig. 7</em>.
    </p>
    <figure>
      <img
        src="imgs/Lab%205%20Report%20bf4998b43c0c4d6fa686559fbbb64070/variance.png"
        alt="variance.png"
      />
      <figcaption aria-hidden="true">variance.png</figcaption>
    </figure>
    <figure>
      <img
        src="imgs/Lab%205%20Report%20bf4998b43c0c4d6fa686559fbbb64070/45ab51b8-4a6f-4544-93d9-3e2884ca86c4.png"
        alt="variance2.png"
      />
      <figcaption aria-hidden="true">variance2.png</figcaption>
    </figure>
    <p>
      <strong><em>Fig. 7.</em> Variance over time in simulation.</strong> On the
      left you see a big spike in the variance and then it gradually converges
      to a stable value ~0.1 on the right. The x axis represents time and the y
      axis represents variance.
    </p>
    <h3 id="b.-performance-in-the-real-world">
      B. Performance in the Real World
    </h3>
    <p><em>Author: Bradyn, Ty</em></p>
    <p>
      In order to get our localization algorithm working on the robot, we had to
      make a few changes to the params.yaml file in order to make everything
      publish to the correct topics. After debugging issues with axes, the
      localization algorithm was working. An initial guess was given for the
      position of the robot in RVIZ. Immediately, we saw the pose of the best
      guess change slightly. After testing with poor initial estimates, we saw
      that the algorithm would move the best guess to a more reasonable
      location. This demonstrates an insensitivity to small errors in initial
      guesses for the pose, as shown in <em>Fig. 8</em>.
    </p>
    <figure>
      <img
        src="imgs/real_world.png"
        alt="Screenshot 2024-04-10 at 9.11.58 PM.png"
      />
      <figcaption aria-hidden="true">
        Screenshot 2024-04-10 at 9.11.58 PM.png
      </figcaption>
    </figure>
    <p>
      <a
        href="https://drive.google.com/file/d/1ioPuT2ldmXYHNusVVRQOF-l_ed7SAjBA/view?usp=drive_link"
        >https://drive.google.com/file/d/1ioPuT2ldmXYHNusVVRQOF-l_ed7SAjBA/view?usp=drive_link</a
      >
    </p>
    <p>
      <strong
        ><em>Fig. 8.</em> Side by side of video of robot driving in Stata
        basement and localization running in RVIZ.</strong
      >
      Note how well the two correlate with each other besides the small time
      lag.
    </p>
    <p>
      Once the initial position stabilized, we began using teleop to control the
      robot. Despite the lag in the localization algorithm’s output, it
      maintained a stable and reasonable pose most of the time. Issues did arise
      in scenarios like navigating through the narrow space near the lab
      entrance and in cluttered areas. Nevertheless, it was encouraging to
      observe that the estimated pose did not worsen over time during regular
      driving.
    </p>
    <p>
      Using teleop mode we were also able to test the variance, the results of
      which are shown in <em>Fig. 9</em>. Similar to the variance in
      <em>Section 3.A</em>, it starts out very high when the particles are
      initially dispersed then it quickly collapses to a stable value. The
      stable variance in the real world and in simulation are almost the same
      which tells us that our particles are converging properly on hardware.
    </p>
    <figure>
      <img
        src="imgs/simulated_variance.png"
        alt="Screenshot 2024-04-10 at 9.14.38 PM.png"
      />
      <figcaption aria-hidden="true">
        Screenshot 2024-04-10 at 9.14.38 PM.png
      </figcaption>
    </figure>
    <p>
      <a
        href="https://drive.google.com/file/d/1h4kKK38vpNltE-2wNhMxgGtICfhzLlww/view?usp=drive_link"
        >https://drive.google.com/file/d/1h4kKK38vpNltE-2wNhMxgGtICfhzLlww/view?usp=drive_link</a
      >
    </p>
    <p>
      <strong><em>Fig. 9.</em> Variance over time in physical robot.</strong>
      The following graph shows variance over time as the physical robot drives
      in the MIT Stata basement. It starts with a huge spike in variance during
      initialization then quickly converges to ~0.1 and remains there throughout
      the run. The x axis represents time and the y axis represents variance.
    </p>
    <h2 id="iv.-conclusion">IV. Conclusion</h2>
    <p><em>Author: Rulan Gu</em></p>
    <p>
      This paper presents our implementation for location estimation of our
      autonomous robot. Our robot is now capable of estimating where it is on a
      map of the Stata basement. To do this, we averaged a cloud of particles
      representing possible robot locations to get its estimated location. We
      updated our particle cloud by using odometry to calculate the next
      position of the particles and by using laser scan data to calculate the
      probability of the robot being at each particle. In simulation, the
      particle cloud was accurate to the robot’s true location while also
      maintaining a reasonable variance. In the real world, we were able to
      qualitatively observe that the robot’s location estimation was accurate
      based on the estimated pose on the Stata basement map.
    </p>
    <p>
      Further work can still be done on evaluating the accuracy of the robot’s
      real world performance quantitatively. One way to do this would be to run
      the wall-following algorithm, which was designed previously, and the MCL
      algorithm at the same time and see if the MCL algorithm accurately
      estimates the distance from the wall. Additionally, we noticed that the
      particle cloud seems to lag at high speeds, which could be due to the
      models not updating frequently enough to keep up or excessive computations
      being done. On longer stretches with similar walls (such as long straight
      hallways), the more uniform nature of the laser scan data also seemed to
      cause the particle resampling to be ineffective.
    </p>
    <p>
      Overall, our location estimation algorithm has been implemented
      successfully. This useful feature will enable us to implement features
      such as path planning in the future since we can now find our robot’s
      location on the map.
    </p>
    <h3 id="lessons-learned">Lessons Learned</h3>
    <p>
      Each member of the team contributed heavily to the success of the project.
      As a result, we each learned various lessons in both the technical and
      communicative aspects of the lab.
    </p>
    <p><em>Author: Rulan Gu</em></p>
    <p>
      This lab taught me how to use probability to figure out the location of a
      robot on the map. I was able to learn about a very interesting
      localization algorithm. The lab was very challenging and I honestly wish
      we were given a bit more time to do it, but it also taught me how much we
      can get done as a team in just a week.
    </p>
    <p><em>Author: Bradyn Lenning</em></p>
    <p>
      This lab taught me a lot about how effective a team can be when work is
      distributed well. We all got our parts working well, but unsurprisingly
      integration was challenging. Eventually we got everything working together
      though! It was also interesting learning about how much precomputing can
      speed up a program.
    </p>
    <p><em>Author: Aimee Liu</em></p>
    <p>
      This lab taught me much about probability and how to use it to predict
      motion. I was able to practice my skills in transformations and
      mathematically solving for next states with matrices.
    </p>
    <p>
      I also learned to take initiative, make plans, and delegate tasks to meet
      deadlines. Organization, consistency, and frequent communication greatly
      improves the workflow of the team.
    </p>
    <p><em>Author: Sruthi Parthasarathi</em></p>
    <p>
      I particularly enjoyed this lab because a large part of designing our
      systems in simulation required us to be thoughtful about increasing
      robustness to errors in the real world, rather than accounting for them
      after integration with the robot.
    </p>
    <p>
      I also gained more experience debugging with the hardware, and we
      collectively learned to delegate tasks in a way that aligned with the
      interests and strengths of the team members.
    </p>
    <p><em>Author: Tyrin Todd</em></p>
    <p>
      During this lab I enjoyed learning about how to think about error and
      randomness in robotics. I also thought it was very interesting that this
      was the first lab where latency is a factor and during our implementation
      we had to learn how to make the tradeoff between latency and accuracy.
    </p>
    <h2 id="references">References</h2>
    <p>
      [1] “Lab 5: Monte Carlo Localization (MCL)” Accessed: Apr. 10, 2024.
      [Online]. Available:
      <a href="https://github.com/mit-rss/localization"
        >https://github.com/mit-rss/localization</a
      >
    </p>
  </body>
</html>
